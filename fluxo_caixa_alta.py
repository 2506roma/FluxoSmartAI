# O c√≥digo devera listar as maiores empresas do brasil
import os # Necess√°rio para acessar as variaveis 
from dotenv import load_dotenv # Acesso aos Arquivos 
import itertools # Loop entre as chaves
import spacy #Processamento de Lingua Natural (NPL)
import serpapi # API para Busca
from collections import defaultdict # Dic especial 
import unicodedata # Remove Acentos
from rapidfuzz import fuzz, process # Similidade em % Das Strings
import aiohttp # Requisi√ß√µes HTTP ass√≠ncronas
import asyncio # Execu√ß√£o ass√≠ncrona 
from bs4 import BeautifulSoup # Analisa os Doc em HTML e XML
from concurrent.futures import ThreadPoolExecutor # Rodando tarefa simultaneamente 

load_dotenv() # Carregando as vari√°veis do arquivo .env 
api_keys = [os.getenv("SERPAPI_KEY1"), os.getenv("SERPAPI_KEY2")] # Armazenando a chave

# Verifica se as chaves foram carregadas corretamente
api_keys = [key for key in api_keys if key]  # Remove valores None

if not api_keys:
    raise ValueError("‚ö† ERRO: Nenhuma chave da API SerpAPI foi encontrada no arquivo .env!")

# Interador infinito para alterar entre as chaves 
api_key_cycle = itertools.cycle(api_keys)

# Carregar o modelo de linguagem 
nlp = spacy.load("pt_core_news_lg")

# Buscando os links
def buscar_links(pesquisa):    
    #Fontes mais confi√°veis
    fontes_primarias = ["SEBRAE", "IBGE", "DELOITTE", "SERASA EXPERIAN", "BACEN", "MINIST√âRIO DA ECONOMIA", "PWC", "MCKINSEY", "BCG", "Valor Econ√¥mico"]
    fontes_primarias = [tratamento_texto(fonte) for fonte in fontes_primarias] # Padronizando: Removendo acentos e transformando tudo em mai√∫sculas
    similaridade = 75 

    api_key = next(api_key_cycle) # Alternando as chaves 
    client = serpapi.Client(api_key=api_key) # Criando uma chave de acesso client

    params = {
        "q": pesquisa,
        "location": "Brazil",
        "hl": "pt",
        "gl": "br",
        "google_domain": "google.com"
    }
    try:
        result = client.search(params)
        if not result:
            print(f"‚ö† Nenhum resultado encontrado para '{pesquisa}'")
            return [], []
        busca = []
        links = []
        
        # Filtragem das informa√ß√µes  
        if "organic_results" in result:
            for item in result["organic_results"]:
                if "link" in item and "source" in item: # criterios para filtragem 
                    link = item ["link"]
                    source = tratamento_texto(item["source"]) # Padroniza a descri√ß√£o   
                    source_o = source
                    # Encontrando a melhor correspond√™ncia na lista 
                    melhor_correspondencia, score, _ = process.extractOne(source, fontes_primarias, scorer=fuzz.partial_ratio)

                    if score >= similaridade:
                        links.append(item['link'])
                        busca.append({
                            "pesquisa": pesquisa,
                            "original": source_o,
                            "source": melhor_correspondencia,
                            "link": link
                        })
        else:
            print("‚ö† A chave 'organic_results' n√£o foi encontrada no resultado da pesquisa!")
        return busca, links  # Retorna os links filtrados
    except Exception as e:
        print(f"Erro ao buscar por '{pesquisa}': {e}")
        return [], []
    
    # Lista com setores validos 
setores_tratados = defaultdict(set)
setores_validos = [
    # Agricultura e Pecu√°ria
    "Agroneg√≥cio", "Agropecu√°ria", "Pecu√°ria", "Produ√ß√£o Animal", "Cultivo de Gr√£os",
    "Produ√ß√£o Florestal", "Pesca", "Aquicultura",
 
    # Ind√∫stria
    "Ind√∫stria", "Ind√∫stria Automobil√≠stica", "Ind√∫stria Naval", "Ind√∫stria Aeroespacial",
    "Ind√∫stria Farmac√™utica", "Ind√∫stria Qu√≠mica", "Ind√∫stria T√™xtil", "Ind√∫stria de Bebidas",
    "Ind√∫stria de Cosm√©ticos", "Ind√∫stria Metal√∫rgica", "Ind√∫stria de Papel e Celulose",
    "Ind√∫stria Eletroeletr√¥nica", "Ind√∫stria de Embalagens", "Ind√∫stria de Pl√°sticos",

    # Constru√ß√£o e Infraestrutura
    "Constru√ß√£o", "Constru√ß√£o Civil", "Engenharia Civil", "Obras de Infraestrutura",
    "Arquitetura", "Materiais de Constru√ß√£o",

    # Energia e Recursos Naturais
    "Energia", "Energia Renov√°vel", "Energia Solar", "Energia E√≥lica", "Petr√≥leo e G√°s",
    "Minera√ß√£o", "Extra√ß√£o de Minerais", "Explora√ß√£o de G√°s Natural",

    # Com√©rcio e Varejo
    "Varejo", "Atacado","Com√©rcio" "Com√©rcio Exterior", "E-commerce", "Supermercados", 
    "Lojas de Departamento", "Com√©rcio Eletr√¥nico",

    # Tecnologia e Inova√ß√£o
    "Tecnologia", "Startups", "Desenvolvimento de Software", "Hardware", "Intelig√™ncia Artificial",
    "Big Data", "Ciberseguran√ßa", "Computa√ß√£o em Nuvem", "Blockchain",

    # Sa√∫de e Bem-Estar
    "Sa√∫de", "Medicina", "Hospitalar", "Farmac√™utico", "Odontologia", "Veterin√°ria",
    "Est√©tica e Beleza", "Academias e Fitness",

    # Educa√ß√£o
    "Educa√ß√£o", "Ensino Superior", "Ensino Fundamental e M√©dio", "Educa√ß√£o Profissional",
    "Cursos Online", "E-learning",

    # Transporte e Log√≠stica
    "Transporte", "Log√≠stica", "Avia√ß√£o", "Aeroportos", "Transporte Ferrovi√°rio",
    "Transporte Mar√≠timo", "Correios e Entregas R√°pidas",

    # Finan√ßas e Seguros
    "Finan√ßas", "Banc√°rio", "Seguros", "Investimentos", "Mercado Financeiro", "Fintechs",
    "Cart√µes de Cr√©dito", "Pagamentos Digitais",

    # Servi√ßos e Consultoria
    "Servi√ßos", "Consultoria Empresarial", "Recursos Humanos", "Coworking",
    "Marketing Digital", "Publicidade", "Assessoria de Imprensa",

    # Turismo e Entretenimento
    "Turismo", "Hotelaria", "Entretenimento", "Restaurantes", "Eventos e Shows",
    "Cinema", "Esportes", "Parques Tem√°ticos",

    # Setor P√∫blico e ONGs
    "Setor P√∫blico", "Administra√ß√£o P√∫blica", "ONGs", "Terceiro Setor",
    "Defesa e Seguran√ßa", "For√ßas Armadas", "Educa√ß√£o P√∫blica",

    # Comunica√ß√£o e M√≠dia
    "Telecomunica√ß√µes", "TV e R√°dio", "Jornalismo", "Publicidade e Propaganda",
    "Streaming", "Redes Sociais",

    # Economia Criativa
    "Ind√∫stria Criativa", "Design", "Moda", "Artes Visuais", "M√∫sica", "Produ√ß√£o Audiovisual",

    # Outros
    "Setor Jur√≠dico", "Advocacia", "Auditoria e Compliance", "Gest√£o Ambiental",
    "Sustentabilidade", "Economia Circular"
]

# Fun√ß√£o tratamento texto
def tratamento_texto(texto: str) -> str:
    if not isinstance(texto, str):
        return ''
    return "".join(
            c for c in unicodedata.normalize('NFKD', texto.strip().upper())
            if unicodedata.category(c)!='Mn'
        )    

# Criando Dicion√°rio com Setores Tratado
for setor in setores_validos:
    chave = tratamento_texto(setor)
    setores_tratados[chave].add(setor)



# Extrair Titulos e Listas 
async def extrair_setores(url: str):
    headers = {"User-Agent": "Mozilla/5.0"}
    setores_detectados = set()  
    try:
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
            async with session.get(url, headers=headers) as response:
                if response.status != 200:
                    print(f"Erro {response.status} ao acessar {url}")
                    return set ()
        


                soup = BeautifulSoup(await response.text(), "html.parser")
                extracoes = {
                    "h1": [tag.text.strip() for tag in soup.find_all('h1')],
                    "h2": [tag.text.strip() for tag in soup.find_all('h2')],
                    "h3": [tag.text.strip() for tag in soup.find_all('h3')],
                    "li": [tag.text.strip() for tag in soup.find_all('li')],
                    "p": [tag.text.strip() for tag in soup.find_all('p')],
                    "strong": [tag.text.strip() for tag in soup.find_all('strong')],
                    "b": [tag.text.strip() for tag in soup.find_all('b')],
                    "td": [tag.text.strip() for tag in soup.find_all('td')],
                }
                #tags_extraidos = soup.select("h1, h2, h3, li, p, strong, b , td")
                tags_extraidos = sum(extracoes.values(), [])

                texto = " ".join([p.get_text() for p in soup.find_all("p")]) # Tratamento e solita√ß√µes. (for em p solicita todas as tags P ) (p.gat_taxt -> solicita√ß√£o de cada texto da tag P) (" ".join solicita que cada string seja armazeenada na variavel com espa√ßo)
                texto = tratamento_texto(texto)

                print(f"\nüîé [DEBUG] Texto extra√≠do de {url}: {texto[:500]}...")  # Mostra um trecho do texto

                setores_detectados = set()
                for tag in tags_extraidos:
                    texto_solicitado = unicodedata.normalize("NFKD", tag.strip().upper())  # Normaliza diretamente a string
                    if not texto_solicitado:
                        print("Erro ao solicitar texto")
                        continue
                    texto_normalizado = tratamento_texto(texto_solicitado)
                    
                    if texto_normalizado in setores_tratados:
                        print(f"‚úÖ Setor encontrado: {setores_tratados[texto_normalizado]}")  # Confirma se encontrou o setor
                        setores_detectados.update(setores_tratados[texto_normalizado])


                return setores_detectados
    except aiohttp.ClientError as e:
        print(f"Erro ao acessar a URL {url}: {e}")
        return set()

# Indificando os setores ORG e MISC
def identificando_setores(texto: str):
    if not texto:
        return set()
    setores_detectados = set()
    doc = nlp(texto)
    for ent in doc.ents:
        if ent.label_ in ["ORG", "MISC"]:
            setores_detectados.add(ent.text)
    return setores_detectados

# Lista de pesquisa
lista_pesquisas = [
        "Quais s√£o os setores que mais movimentam a economia das PMEs no Brasil em faturamento?" , 
        "Quais s√£o os setores que mais faturam no Brasil atualmente, considerando empresas de todos os portes? ",
        "Quais setores tiveram o maior crescimento de receita no Brasil nos √∫ltimos dois anos"
    ]

# processando os setores 
async def processar_setores(lista_links_validos):
    setores_identificados = set()
    tarefas = [extrair_setores(url) for url in lista_links_validos]
    resultados = await asyncio.gather(*tarefas, return_exceptions=True)


    for setores in resultados:
        print(f"\nüîπ [DEBUG] Setores extra√≠dos: {setores}")  # Verifica o que foi encontrado

        if isinstance(setores, set):
            setores_identificados.update(setores)
        # else:
        #     print(f"‚ö† Aviso: Um dos resultados n√£o retornou um conjunto v√°lido: {setores}
                  
    print("\nüìå **Setores Identificados:**")
    print("-" * 50)
    for setor in sorted(setores_identificados):
        print(f"‚úÖ {setor}")
    print("-" * 50)

        
    

if __name__ == "__main__":
    # Apenas ser√° executado se rodar diretamente este arquivo
    print("Executando setores_api.py diretamente!")

    with ThreadPoolExecutor(max_workers=2) as executor:
        resultados = list(executor.map(buscar_links, lista_pesquisas))

    # Separando os resultados da busca e os links coletados
    busca_resultados = []
    lista_links = []

    for busca, links in resultados:
        busca_resultados.extend(busca)
        lista_links.extend(links)

    # Remover duplicatas nos links
    lista_links = list(set(lista_links))

    # Exibir os links encontrados
    print("\nüîé **Links encontrados:**")
    for link in lista_links:
        print(f"‚úÖ {link}")

    # Processar setores identificados
    lista_links_validos = [url for url in lista_links if url.startswith("http")]

    if lista_links_validos:
        asyncio.run(processar_setores(lista_links_validos))
    else:
        print("‚ö† Nenhum link v√°lido encontrado para processar os setores!")