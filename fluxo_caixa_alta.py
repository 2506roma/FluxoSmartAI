# O cÃ³digo devera listar as maiores empresas do brasil
import os # NecessÃ¡rio para acessar as variaveis 
from dotenv import load_dotenv # Acesso aos Arquivos 
import itertools # Loop entre as chaves
import spacy #Processamento de Lingua Natural (NPL)
import serpapi # API para Busca
from collections import defaultdict # Dic especial 
import unicodedata # Remove Acentos
from rapidfuzz import fuzz, process # Similidade em % Das Strings
import aiohttp # RequisiÃ§Ãµes HTTP assÃ­ncronas
import asyncio # ExecuÃ§Ã£o assÃ­ncrona 
from bs4 import BeautifulSoup # Analisa os Doc em HTML e XML
from concurrent.futures import ThreadPoolExecutor # Rodando tarefa simultaneamente 

load_dotenv() # Carregando as variÃ¡veis do arquivo .env 
api_keys = [os.getenv("SERPAPI_KEY1"), os.getenv("SERPAPI_KEY2")] # Armazenando a chave

# Verifica se as chaves foram carregadas corretamente
api_keys = [key for key in api_keys if key]  # Remove valores None

if not api_keys:
    raise ValueError("âš  ERRO: Nenhuma chave da API SerpAPI foi encontrada no arquivo .env!")

# Interador infinito para alterar entre as chaves 
api_key_cycle = itertools.cycle(api_keys)

# Carregar o modelo de linguagem 
nlp = spacy.load("pt_core_news_lg")

# Buscando os links
def buscar_links(pesquisa):    
    #Fontes mais confiÃ¡veis
    fontes_primarias = ["SEBRAE", "IBGE", "DELOITTE", "SERASA EXPERIAN", "BACEN", "MINISTÃ‰RIO DA ECONOMIA", "PWC", "MCKINSEY", "BCG", "Valor EconÃ´mico"]
    fontes_primarias = [tratamento_texto(fonte) for fonte in fontes_primarias] # Padronizando: Removendo acentos e transformando tudo em maiÃºsculas
    similaridade = 75 

    api_key = next(api_key_cycle) # Alternando as chaves 
    client = serpapi.Client(api_key=api_key) # Criando uma chave de acesso client

    params = {
        "q": pesquisa,
        "location": "Brazil",
        "hl": "pt",
        "gl": "br",
        "google_domain": "google.com"
    }
    try:
        result = client.search(params)
        if not result:
            print(f"âš  Nenhum resultado encontrado para '{pesquisa}'")
            return [], []
        busca = []
        links = []
        
        # Filtragem das informaÃ§Ãµes  
        if "organic_results" in result:
            for item in result["organic_results"]:
                if "link" in item and "source" in item: # criterios para filtragem 
                    link = item ["link"]
                    source = tratamento_texto(item["source"]) # Padroniza a descriÃ§Ã£o   
                    source_o = source
                    # Encontrando a melhor correspondÃªncia na lista 
                    melhor_correspondencia, score, _ = process.extractOne(source, fontes_primarias, scorer=fuzz.partial_ratio)

                    if score >= similaridade:
                        links.append(item['link'])
                        busca.append({
                            "pesquisa": pesquisa,
                            "original": source_o,
                            "source": melhor_correspondencia,
                            "link": link
                        })
        else:
            print("âš  A chave 'organic_results' nÃ£o foi encontrada no resultado da pesquisa!")
        return busca, links  # Retorna os links filtrados
    except Exception as e:
        print(f"Erro ao buscar por '{pesquisa}': {e}")
        return [], []
    
    # Lista com setores validos 
setores_tratados = defaultdict(set)
setores_validos = [
    # Agricultura e PecuÃ¡ria
    "AgronegÃ³cio", "AgropecuÃ¡ria", "PecuÃ¡ria", "ProduÃ§Ã£o Animal", "Cultivo de GrÃ£os",
    "ProduÃ§Ã£o Florestal", "Pesca", "Aquicultura",
 
    # IndÃºstria
    "IndÃºstria", "IndÃºstria AutomobilÃ­stica", "IndÃºstria Naval", "IndÃºstria Aeroespacial",
    "IndÃºstria FarmacÃªutica", "IndÃºstria QuÃ­mica", "IndÃºstria TÃªxtil", "IndÃºstria de Bebidas",
    "IndÃºstria de CosmÃ©ticos", "IndÃºstria MetalÃºrgica", "IndÃºstria de Papel e Celulose",
    "IndÃºstria EletroeletrÃ´nica", "IndÃºstria de Embalagens", "IndÃºstria de PlÃ¡sticos",

    # ConstruÃ§Ã£o e Infraestrutura
    "ConstruÃ§Ã£o", "ConstruÃ§Ã£o Civil", "Engenharia Civil", "Obras de Infraestrutura",
    "Arquitetura", "Materiais de ConstruÃ§Ã£o",

    # Energia e Recursos Naturais
    "Energia", "Energia RenovÃ¡vel", "Energia Solar", "Energia EÃ³lica", "PetrÃ³leo e GÃ¡s",
    "MineraÃ§Ã£o", "ExtraÃ§Ã£o de Minerais", "ExploraÃ§Ã£o de GÃ¡s Natural",

    # ComÃ©rcio e Varejo
    "Varejo", "Atacado","ComÃ©rcio" "ComÃ©rcio Exterior", "E-commerce", "Supermercados", 
    "Lojas de Departamento", "ComÃ©rcio EletrÃ´nico",

    # Tecnologia e InovaÃ§Ã£o
    "Tecnologia", "Startups", "Desenvolvimento de Software", "Hardware", "InteligÃªncia Artificial",
    "Big Data", "CiberseguranÃ§a", "ComputaÃ§Ã£o em Nuvem", "Blockchain",

    # SaÃºde e Bem-Estar
    "SaÃºde", "Medicina", "Hospitalar", "FarmacÃªutico", "Odontologia", "VeterinÃ¡ria",
    "EstÃ©tica e Beleza", "Academias e Fitness",

    # EducaÃ§Ã£o
    "EducaÃ§Ã£o", "Ensino Superior", "Ensino Fundamental e MÃ©dio", "EducaÃ§Ã£o Profissional",
    "Cursos Online", "E-learning",

    # Transporte e LogÃ­stica
    "Transporte", "LogÃ­stica", "AviaÃ§Ã£o", "Aeroportos", "Transporte FerroviÃ¡rio",
    "Transporte MarÃ­timo", "Correios e Entregas RÃ¡pidas",

    # FinanÃ§as e Seguros
    "FinanÃ§as", "BancÃ¡rio", "Seguros", "Investimentos", "Mercado Financeiro", "Fintechs",
    "CartÃµes de CrÃ©dito", "Pagamentos Digitais",

    # ServiÃ§os e Consultoria
    "ServiÃ§os", "Consultoria Empresarial", "Recursos Humanos", "Coworking",
    "Marketing Digital", "Publicidade", "Assessoria de Imprensa",

    # Turismo e Entretenimento
    "Turismo", "Hotelaria", "Entretenimento", "Restaurantes", "Eventos e Shows",
    "Cinema", "Esportes", "Parques TemÃ¡ticos",

    # Setor PÃºblico e ONGs
    "Setor PÃºblico", "AdministraÃ§Ã£o PÃºblica", "ONGs", "Terceiro Setor",
    "Defesa e SeguranÃ§a", "ForÃ§as Armadas", "EducaÃ§Ã£o PÃºblica",

    # ComunicaÃ§Ã£o e MÃ­dia
    "TelecomunicaÃ§Ãµes", "TV e RÃ¡dio", "Jornalismo", "Publicidade e Propaganda",
    "Streaming", "Redes Sociais",

    # Economia Criativa
    "IndÃºstria Criativa", "Design", "Moda", "Artes Visuais", "MÃºsica", "ProduÃ§Ã£o Audiovisual",

    # Outros
    "Setor JurÃ­dico", "Advocacia", "Auditoria e Compliance", "GestÃ£o Ambiental",
    "Sustentabilidade", "Economia Circular"
]

# FunÃ§Ã£o tratamento texto
def tratamento_texto(texto: str) -> str:
    if not isinstance(texto, str):
        return ''
    return "".join(
            c for c in unicodedata.normalize('NFKD', texto.strip().upper())
            if unicodedata.category(c)!='Mn'
        )    

# Criando DicionÃ¡rio com Setores Tratado
for setor in setores_validos:
    chave = tratamento_texto(setor)
    setores_tratados[chave].add(setor)



# Extrair Titulos e Listas 
async def extrair_setores(url: str):
    headers = {"User-Agent": "Mozilla/5.0"}
    setores_detectados = set()  
    try:
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
            async with session.get(url, headers=headers) as response:
                if response.status != 200:
                    print(f"Erro {response.status} ao acessar {url}")
                    return set ()
        


                soup = BeautifulSoup(await response.text(), "html.parser")
                extracoes = {
                    "h1": [tag.text.strip() for tag in soup.find_all('h1')],
                    "h2": [tag.text.strip() for tag in soup.find_all('h2')],
                    "h3": [tag.text.strip() for tag in soup.find_all('h3')],
                    "li": [tag.text.strip() for tag in soup.find_all('li')],
                    "p": [tag.text.strip() for tag in soup.find_all('p')],
                    "strong": [tag.text.strip() for tag in soup.find_all('strong')],
                    "b": [tag.text.strip() for tag in soup.find_all('b')],
                    "td": [tag.text.strip() for tag in soup.find_all('td')],
                }
                #tags_extraidos = soup.select("h1, h2, h3, li, p, strong, b , td")
                tags_extraidos = sum(extracoes.values(), [])

                texto = " ".join([p.get_text() for p in soup.find_all("p")]) # Tratamento e solitaÃ§Ãµes. (for em p solicita todas as tags P ) (p.gat_taxt -> solicitaÃ§Ã£o de cada texto da tag P) (" ".join solicita que cada string seja armazeenada na variavel com espaÃ§o)
                texto = tratamento_texto(texto)

                print(f"\nðŸ”Ž [DEBUG] Texto extraÃ­do de {url}: {texto[:500]}...")  # Mostra um trecho do texto

                setores_detectados = set()
                for tag in tags_extraidos:
                    texto_solicitado = unicodedata.normalize("NFKD", tag.strip().upper())  # Normaliza diretamente a string
                    if not texto_solicitado:
                        print("Erro ao solicitar texto")
                        continue
                    texto_normalizado = tratamento_texto(texto_solicitado)
                    
                    if texto_normalizado in setores_tratados:
                        print(f"âœ… Setor encontrado: {setores_tratados[texto_normalizado]}")  # Confirma se encontrou o setor
                        setores_detectados.update(setores_tratados[texto_normalizado])


                return setores_detectados
    except aiohttp.ClientError as e:
        print(f"Erro ao acessar a URL {url}: {e}")
        return set()

# Indificando os setores ORG e MISC
def identificando_setores(texto: str):
    if not texto:
        return set()
    setores_detectados = set()
    doc = nlp(texto)
    for ent in doc.ents:
        if ent.label_ in ["ORG", "MISC"]:
            setores_detectados.add(ent.text)
    return setores_detectados

# Lista de pesquisa
lista_pesquisas = [
        "Quais sÃ£o os setores que mais movimentam a economia das PMEs no Brasil em faturamento?" , 
        "Quais sÃ£o os setores que mais faturam no Brasil atualmente, considerando empresas de todos os portes? ",
        "Quais setores tiveram o maior crescimento de receita no Brasil nos Ãºltimos dois anos"
    ]

# processando os setores 
async def processar_setores(lista_links_validos):
    setores_identificados = set()
    tarefas = [extrair_setores(url) for url in lista_links_validos]
    resultados = await asyncio.gather(*tarefas, return_exceptions=True)


    for setores in resultados:
        print(f"\nðŸ”¹ [DEBUG] Setores extraÃ­dos: {setores}")  # Verifica o que foi encontrado

        if isinstance(setores, set):
            setores_identificados.update(setores)
        # else:
        #     print(f"âš  Aviso: Um dos resultados nÃ£o retornou um conjunto vÃ¡lido: {setores}
                  
    print("\nðŸ“Œ **Setores Identificados:**")
    print("-" * 50)
    for setor in sorted(setores_identificados):
        print(f"âœ… {setor}")
    print("-" * 50)

        
    

if __name__ == "__main__":
    # Apenas serÃ¡ executado se rodar diretamente este arquivo
    print("Executando setores_api.py diretamente!")

    with ThreadPoolExecutor(max_workers=2) as executor:
        resultados = list(executor.map(buscar_links, lista_pesquisas))

    # Separando os resultados da busca e os links coletados
    busca_resultados = []
    lista_links = []

    for busca, links in resultados:
        busca_resultados.extend(busca)
        lista_links.extend(links)

    # Remover duplicatas nos links
    lista_links = list(set(lista_links))

    # Exibir os links encontrados
    print("\nðŸ”Ž **Links encontrados:**")
    for link in lista_links:
        print(f"âœ… {link}")

    # Processar setores identificados
    lista_links_validos = [url for url in lista_links if url.startswith("http")]

    if lista_links_validos:
        asyncio.run(processar_setores(lista_links_validos))
    else:
        print("âš  Nenhum link vÃ¡lido encontrado para processar os setores!")